{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from nltk.corpus import names\n",
        "import nltk; nltk.download('stopwords')\n",
        "# NLTK Stop words\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import re\n",
        "\n",
        "from pymorphy2 import MorphAnalyzer\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
        "\n",
        "# Plotting tools\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim  # don't skip this\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "n_features = 1000\n",
        "n_components = 16\n",
        "n_top_words = 20\n",
        "\n",
        "\n",
        "def print_top_words(model, feature_names, n_top_words):\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        message = \"Topic #%d: \" % topic_idx\n",
        "        message += \" \".join([feature_names[i]\n",
        "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
        "        print(message)\n",
        "    print()\n",
        "\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "7jGNmRLntLwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "import dill"
      ],
      "metadata": {
        "id": "vgyiX_E0tN0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('max_colwidth', 120)\n",
        "pd.set_option('display.width', 500)"
      ],
      "metadata": {
        "id": "JHgH9GQktPPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"Заявки в чатбот.csv\", encoding = 'utf-16', sep=\";\")\n",
        "data.tail(20)"
      ],
      "metadata": {
        "id": "ivdNqnZ_tTUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextImputer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, key, value):\n",
        "        self.key = key\n",
        "        self.value = value\n",
        "        \n",
        "    def get_stopwords(self):\n",
        "        russian_stopwords = stopwords.words(\"russian\")\n",
        "        df_sw = pd.read_csv('stopwords.csv', encoding = 'utf-8', sep=\";\")\n",
        "        for index, row in df_sw.iterrows():\n",
        "            russian_stopwords.append(row['stopword'])\n",
        "        return russian_stopwords\n",
        "        \n",
        "    def to_lemmatize2(self, df, key):\n",
        "        all_word_str = \" \".join(df[key])\n",
        "        all_word_list = all_word_str.split()\n",
        "        all_unique_word = pd.Series(all_word_list).unique()\n",
        "        lemmatized_word_dict = {}\n",
        "        lemmatizer = MorphAnalyzer()\n",
        "        for word in all_unique_word:\n",
        "            lemmatized_word_dict[word] = lemmatizer.normal_forms(word)[0]\n",
        "        lemm_func = lambda text: ' '.join([lemmatized_word_dict[word] for word in text.split()])\n",
        "        df[key] = df[key].apply(lemm_func)\n",
        "        return df, all_unique_word\n",
        "    \n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    def transform(self, X):\n",
        "\n",
        "        X[self.key] = X[self.key].replace('—','-')\n",
        "        \n",
        "        #1. удаляем пунктуацию\n",
        "        deleted_symbols = r'[\\\\\\\\\\'[\\]!\"$%&()*+,-./:;<=>?№@^_`{|}~«»\\n]'  \n",
        "        func = lambda text : re.sub(deleted_symbols, ' ', str(text))\n",
        "        X[self.key] = X[self.key].apply(func)\n",
        "        \n",
        "        #2. удалим смайлики\n",
        "        emoji_pattern = re.compile(\"[\"\n",
        "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "        func = lambda text : re.sub(emoji_pattern, ' ', str(text))\n",
        "        X[self.key] = X[self.key].apply(func)\n",
        "        \n",
        "        #3. удалим отдельно стоящие цифры\n",
        "        func = lambda text : ' '.join([elem for elem in str(text).split(' ') if elem.isdigit() == False])   \n",
        "        X[self.key] = X[self.key].apply(func)\n",
        "        \n",
        "        #4. приводим к нижнему регистру\n",
        "        X[self.key] = X[self.key].apply(lambda text : text.lower())\n",
        "        \n",
        "        #5. лемматизация (приводим слова к начальной форме)\n",
        "        X, _ = self.to_lemmatize2(X, self.key)\n",
        "        \n",
        "        #6. удаляем стоп слова\n",
        "        sw = self.get_stopwords()\n",
        "        func = lambda text : ' '.join([elem for elem in str(text).split(' ') if elem not in sw and not elem in ['nan', np.nan]])   \n",
        "        X[self.key] = X[self.key].apply(func)\n",
        "        \n",
        "        return X \n",
        "    \n",
        "class ColumnSelector(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Transformer to select a single column from the data frame to perform additional transformations on\n",
        "    \"\"\"\n",
        "    def __init__(self, key):\n",
        "        self.key = key\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        #приведем к виду списка списков, потом этот список списков пойдет в модель LDA\n",
        "        lst = X[self.key].to_list()\n",
        "        texts = []\n",
        "        for i in range(len(lst)):    \n",
        "            texts.append(lst[i].split(' '))\n",
        "        return texts"
      ],
      "metadata": {
        "id": "xG5L0IL5tPWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "description = Pipeline([\n",
        "                ('imputer', TextImputer('mesTExt', '')),\n",
        "                ('selector', ColumnSelector(key='mesTExt'))\n",
        "            ])"
      ],
      "metadata": {
        "id": "1G8Y1SCxtWnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = Pipeline([('description', description),\n",
        "    ('tfidf_vectorizer', TfidfVectorizer(max_df=0.95, min_df=2, analyzer=lambda x: x, \n",
        "                                   max_features=n_features,\n",
        "                                   stop_words='english')),\n",
        "    ('lda', LatentDirichletAllocation(n_components=n_components, max_iter=50,\n",
        "                                learning_method='online',\n",
        "                                learning_offset=50.,\n",
        "                                random_state=0)),\n",
        "])\n",
        "\n",
        "model = pipeline.fit(data.iloc[:])"
      ],
      "metadata": {
        "id": "ArMAXA6duxpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline.get_params().keys()"
      ],
      "metadata": {
        "id": "u2dcnmCXuzBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.iloc[:10]"
      ],
      "metadata": {
        "id": "YPxpzBMyu06z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_preds = pd.DataFrame(pipeline.transform(data.iloc[:10]))\n",
        "test_preds"
      ],
      "metadata": {
        "id": "7Apsjxqsu2KV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(pipeline.transform(data.iloc[:10]))"
      ],
      "metadata": {
        "id": "0abkBP4U5ETu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_preds.to_csv(\"test_preds.csv\")"
      ],
      "metadata": {
        "id": "H88aSs4i5Gyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nTopics in LDA model:\")\n",
        "tf_feature_names = pipeline.steps[1][1].get_feature_names()\n",
        "print_top_words(pipeline.steps[2][1], tf_feature_names, n_top_words)"
      ],
      "metadata": {
        "id": "HZLjAviq5LyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.iloc[1]"
      ],
      "metadata": {
        "id": "WLdZ1OMy5NXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "6Llm8rCh5Ouf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tf_feature_names)"
      ],
      "metadata": {
        "id": "IA-rDBE_5WzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"lda_pipeline.dill\", \"wb\") as f:\n",
        "    dill.dump(pipeline, f)"
      ],
      "metadata": {
        "id": "lTD03TQt5YeH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}